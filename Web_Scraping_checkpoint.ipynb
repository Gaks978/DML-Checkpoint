{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPR1EisUhUPMR+YulR4oY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaks978/DML-Checkpoint/blob/main/Web_Scraping_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What You're Aiming For\n",
        "\n",
        "The objective is to automate the extraction of HTML content, article titles, text, and internal links from Wikipedia pages into a consolidated function that accepts any Wikipedia URL for efficient data retrieval and processing.\n",
        "\n",
        "\n",
        "Instructions\n",
        "\n",
        "Create a Python script to automate data extraction from Wikipedia pages. The script will retrieve HTML content, extract article titles and text, collect internal links, and consolidate these tasks into one function that accepts a Wikipedia URL. This will be tested on a specific Wikipedia page to validate functionality.\n",
        "\n",
        "1) Write a function to Get and parse html content from a Wikipedia page\n",
        "\n",
        "2) Write a function to Extract article title\n",
        "\n",
        "3) Write a function to Extract article text for each paragraph with their respective\n",
        "\n",
        "headings. Map those headings to their respective paragraphs in the dictionary.\n",
        "\n",
        "4) Write a function to collect every link that redirects to another Wikipedia page\n",
        "\n",
        "5) Wrap all the previous functions into a single function that takes as parameters a Wikipedia link\n",
        "\n",
        "6) Test the last function on a Wikipedia page of your choice"
      ],
      "metadata": {
        "id": "ksEcjgVwQSsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6brYLhcNQG28",
        "outputId": "6d7b56b4-9c2b-43b5-c088-b749d1477251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Web scraping\n",
            "\n",
            "Headings and Content Preview:\n",
            "\n",
            "Introduction:\n",
            "...\n",
            "\n",
            "Number of internal links found: 133\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Step 1: Get and parse HTML content from a Wikipedia page\n",
        "def get_html_content(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 2: Extract article title\n",
        "def extract_title(soup):\n",
        "    return soup.find('h1', {'id': 'firstHeading'}).text.strip()\n",
        "\n",
        "# Step 3: Extract article text and map it to headings\n",
        "def extract_text_by_headings(soup):\n",
        "    content = {}\n",
        "    content_div = soup.find('div', {'class': 'mw-parser-output'})\n",
        "    current_heading = \"Introduction\"\n",
        "    content[current_heading] = []\n",
        "\n",
        "    for element in content_div.find_all(['h2', 'h3', 'p'], recursive=False):\n",
        "        if element.name in ['h2', 'h3']:\n",
        "            span = element.find('span', {'class': 'mw-headline'})\n",
        "            if span:\n",
        "                current_heading = span.text.strip()\n",
        "                content[current_heading] = []\n",
        "        elif element.name == 'p':\n",
        "            text = element.get_text(strip=True)\n",
        "            if text:\n",
        "                content[current_heading].append(text)\n",
        "\n",
        "    # Combine paragraphs for each heading\n",
        "    return {heading: ' '.join(paragraphs) for heading, paragraphs in content.items()}\n",
        "\n",
        "# Step 4: Collect internal Wikipedia links\n",
        "def extract_internal_links(soup):\n",
        "    links = set()\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if href.startswith('/wiki/') and ':' not in href:  # Exclude special pages like \"Category:\", \"File:\", etc.\n",
        "            full_url = urljoin('https://en.wikipedia.org', href)\n",
        "            links.add(full_url)\n",
        "    return list(links)\n",
        "\n",
        "# Step 5: Wrap all into a single function\n",
        "def extract_wikipedia_data(url):\n",
        "    soup = get_html_content(url)\n",
        "    title = extract_title(soup)\n",
        "    content = extract_text_by_headings(soup)\n",
        "    internal_links = extract_internal_links(soup)\n",
        "\n",
        "    return {\n",
        "        'url': url,\n",
        "        'title': title,\n",
        "        'content_by_heading': content,\n",
        "        'internal_links': internal_links\n",
        "    }\n",
        "\n",
        "# Step 6: Test the function\n",
        "if __name__ == \"__main__\":\n",
        "    test_url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
        "    data = extract_wikipedia_data(test_url)\n",
        "\n",
        "    # Displaying just the basics\n",
        "    print(\"Title:\", data['title'])\n",
        "    print(\"\\nHeadings and Content Preview:\")\n",
        "    for heading, text in list(data['content_by_heading'].items())[:3]:  # show top 3 sections only\n",
        "        print(f\"\\n{heading}:\\n{text[:300]}...\")  # preview first 300 characters\n",
        "\n",
        "    print(f\"\\nNumber of internal links found: {len(data['internal_links'])}\")\n"
      ]
    }
  ]
}